{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K_DCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-richie-y/no_eating_no_drinking/blob/master/master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NnW4vCnznxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3mu2m920aHQ",
        "colab_type": "code",
        "outputId": "2936f83e-16db-4243-e91c-634b709d0d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "th.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa41a94a710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H8XvD7P9YxZ",
        "colab_type": "text"
      },
      "source": [
        "$p$ is the pooling size of each maxout layer. \"We use a maxout pool size of 16\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZ1UEU89Sp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pooling size of each maxout layer\n",
        "MAXOUT_POOL_SIZE = 16\n",
        "\n",
        "ENCODING_DIM = 200\n",
        "\n",
        "DROPOUT = 0.3\n",
        "\n",
        "document_sequence_size = 70\n",
        "question_sequence_size = 80\n",
        "\n",
        "# Hidden state dimension of the LSTM\n",
        "HIDDEN_SIZE = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwF6ePTpeefQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 2.1 DOCUMENT AND QUESTION ENCODER\n",
        "\n",
        "# The encoder LSTM.\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, doc_word_vecs, question_word_vecs, hidden_dim, dropout, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # Dimensionality of word vectors.\n",
        "    self.word_vec_dim = doc_word_vecs.size()[1]\n",
        "    assert(self.word_vec_dim == question_word_vecs.size()[1])\n",
        "\n",
        "    # Dimension of the hidden state and cell state (they're equal) of the LSTM\n",
        "    self.lstm = nn.LSTM(self.word_vec_dim, hidden_dim, 1, batch_first=True, bidirectional=False, dropout=dropout)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    return self.lstm(x, hidden)\n",
        "\n",
        "  def generate_initial_hidden_state(self):\n",
        "    return (th.zeros(self.hidden_dim).view(1,batch_size,self.hidden_dim),\n",
        "            th.zeros(self.hidden_dim).view(1,batch_size,self.hidden_dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDdR2NSrZNXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, document_sequence_size, batch_size, dropout=DROPOUT, use_gpu = False):\n",
        "        super(BiLSTMEncoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.document_sequence_size=document_sequence_size\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(3 * hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # first is the hidden h\n",
        "        # second is the cell c\n",
        "        # TODO: Is initialisation zeros or randn? \n",
        "        return (th.zeros(2, self.batch_size, self.hidden_dim),\n",
        "              th.zeros(2, self.batch_size,self.hidden_dim))\n",
        "            #return (Variable(th.zeros(num_layers, self.batch_size, self.hidden_dim).to(device)),\n",
        "            #Variable(th.zeros(num_layers, self.batch_size, self.hidden_dim).to(device)))\n",
        "        \n",
        "\n",
        "    def forward(self, input_BiLSTM):\n",
        "        print('Input shape: ', input_BiLSTM.shape[0], \" \", self.document_sequence_size)\n",
        "        lstm_out, self.hidden = self.lstm(input_BiLSTM.reshape(input_BiLSTM.shape[0], self.document_sequence_size, -1), self.hidden)\n",
        "        \n",
        "        \n",
        "        return lstm_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N55EAxSWZDUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CoattentionModel(nn.Module):\n",
        "    def __init__(self, hidden_dim, dropout_ratio):\n",
        "        super(CoattentionModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        #self.encoder = Encoder(hidden_dim, emb_matrix, dropout_ratio) #Kuba\n",
        "\n",
        "        #self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        #self.fusion_bilstm = FusionBiLSTM(hidden_dim, dropout_ratio)\n",
        "        #self.decoder = DynamicDecoder(hidden_dim, maxout_pool_size, max_dec_steps, dropout_ratio)\n",
        "        #self.dropout = nn.Dropout(p=dropout_ratio)\n",
        "\n",
        "    def forward(self, D, Q):\n",
        "        #Q = self.encoder(q_seq, q_mask) # b x n + 1 x l\n",
        "        #D = self.encoder(d_seq, d_mask)  # B x m + 1 x l\n",
        "        #D=torch.randn(B,hidden_dim, document_sequence_size)#replace with what Kuba wrote #replace size by n+1\n",
        "        #Q=torch.randn(B,hidden_dim, question_sequence_size) #replace with what kuba wrote #replace size by m+1 #B x n + 1 x l\n",
        "\n",
        "        #co attention\n",
        "        \n",
        "        D_T = th.transpose(D, 1, 2) #B x l x m + 1 \n",
        "        L = th.bmm(D_T, Q) # L = B x m + 1 x n + 1\n",
        "        AQ = F.softmax(L, dim=1)\n",
        "        AD_T = F.softmax(L,dim=2)\n",
        "        AD = th.transpose(AD_T, 1, 2) # B x n + 1 x m + 1\n",
        "\n",
        "        CQ = th.bmm(D,AQ) #R l√ó(n+1)\n",
        "        CD = th.bmm(th.cat((Q,CQ),1),AD) # B x 2l x m + 1\n",
        "        C_D_t = th.transpose(CD, 1, 2)  # B x m + 1 x 2l\n",
        "\n",
        "        #fusion BiLSTM\n",
        "        \n",
        "        input_BiLSTM=th.transpose(th.cat((D,CD), 1),1,2)\n",
        "        print(input_BiLSTM.shape)\n",
        "\n",
        "        model = BiLSTMEncoder(hidden_dim, document_sequence_size, B)\n",
        "        return model.forward(input_BiLSTM)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EID6zeni9djU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HighwayMaxoutNetwork(nn.Module):\n",
        "  def __init__(self, maxout_pool_size = MAXOUT_POOL_SIZE, encoding_dim = ENCODING_DIM): \n",
        "    super(HighwayMaxoutNetwork, self).__init__()\n",
        "\n",
        "    self.encoding_dim = encoding_dim\n",
        "    self.maxout_pool_size = MAXOUT_POOL_SIZE\n",
        "\n",
        "    # Don't apply dropout to biases\n",
        "    self.dropout = nn.Dropout(p=DROPOUT)\n",
        "\n",
        "    # W_D := Weights of MLP applied to the coattention encodings of\n",
        "    # the start/end positions, and the current LSTM hidden state (h_i)\n",
        "    # (nn.Linear is an affine transformation y = Wx + b)\n",
        "\n",
        "    # There are 5 * encoding_dim incoming features (u_si-1, u_ei-1, h_i) \n",
        "    # which are vectors containing (2l, 2l, l) elements respectively\n",
        "    # There's l outgoing features (i.e. r)\n",
        "    # There's no bias for this MLP\n",
        "    \n",
        "    # (From OpenReview) random initialisation is used for W's and b's\n",
        "    self.W_D = self.dropout(nn.Parameter(th.randn(self.encoding_dim, 5 * self.encoding_dim)))\n",
        "\n",
        "    # 1st Maxout layer\n",
        "    self.W_1 = self.dropout(nn.Parameter(th.randn(self.maxout_pool_size, self.encoding_dim, 3 * self.encoding_dim)))\n",
        "    self.b_1 = nn.Parameter(th.randn(self.maxout_pool_size, self.encoding_dim))\n",
        "\n",
        "    # 2nd maxout layer\n",
        "    self.W_2 = self.dropout(nn.Parameter(th.randn(self.maxout_pool_size, self.encoding_dim, self.encoding_dim)))\n",
        "    self.b_2 = nn.Parameter(th.randn(self.maxout_pool_size, self.encoding_dim))\n",
        "\n",
        "    # 3rd maxout layer\n",
        "    self.W_3 = self.dropout(nn.Parameter(th.randn(self.maxout_pool_size, 1, 2 * self.encoding_dim)))\n",
        "    self.b_3 = nn.Parameter(th.randn(self.maxout_pool_size, 1))\n",
        "\n",
        "  def forward(self, u_t, h_i, u_si_m_1, u_ei_m_1):\n",
        "\n",
        "    assert(u_t.size()[0] == 2 * self.encoding_dim) \n",
        "    assert(u_t.size()[1] == 1) \n",
        "    assert(h_i.size()[0] == self.encoding_dim)\n",
        "    assert(h_i.size()[1] == 1)\n",
        "    assert(u_si_m_1.size()[0] == 2 * self.encoding_dim)\n",
        "    assert(u_si_m_1.size()[1] == 1)\n",
        "    assert(u_ei_m_1.size()[0] == 2 * self.encoding_dim)\n",
        "    assert(u_ei_m_1.size()[1] == 1)\n",
        "\n",
        "    # r := output of MLP \n",
        "    r = th.tanh(self.W_D.mm(th.cat((h_i, u_si_m_1, u_ei_m_1), 0)))\n",
        "\n",
        "    # m_t_1 := output of 1st maxout layer\n",
        "    m_t_1_beforemaxpool = th.mm(\n",
        "        self.W_1.view(self.maxout_pool_size * self.encoding_dim, \n",
        "                        3 * self.encoding_dim), \n",
        "                        th.cat((u_t, r), 0)\n",
        "    ).view(\n",
        "        self.maxout_pool_size, \n",
        "        self.encoding_dim, \n",
        "        1\n",
        "    ).squeeze() + self.b_1\n",
        "\n",
        "    # The max operation in Eq.9-12 computes the maximum value over the \n",
        "    # first dimension of a tensor\n",
        "    m_t_1 = th.Tensor.max(m_t_1_beforemaxpool, dim=0).values.unsqueeze(dim=1)\n",
        "\n",
        "    m_t_2_beforemaxpool = th.mm(\n",
        "        self.W_2.view(self.maxout_pool_size * self.encoding_dim, \n",
        "                        self.encoding_dim), \n",
        "                        m_t_1\n",
        "    ).view(\n",
        "        self.maxout_pool_size, \n",
        "        self.encoding_dim, \n",
        "        1\n",
        "    ).squeeze() + self.b_2\n",
        "    m_t_2 = th.Tensor.max(m_t_2_beforemaxpool, dim=0).values.unsqueeze(dim=1)\n",
        "\n",
        "    # HMN output\n",
        "    output_beforemaxpool = th.mm(\n",
        "        self.W_3.view(\n",
        "            self.maxout_pool_size, \n",
        "            2 * self.encoding_dim\n",
        "        ), \n",
        "        # Highway connection\n",
        "        th.cat((m_t_1, m_t_2), 0)\n",
        "    ) + self.b_3\n",
        "    \n",
        "    output = th.Tensor.max(output_beforemaxpool, dim=0).values\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkR3seKD9kAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DynamicPointerDecoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DynamicPointerDecoder, self).__init__()\n",
        "    self.hmn_alpha = HighwayMaxoutNetwork()\n",
        "    self.hmn_beta = HighwayMaxoutNetwork()\n",
        "    self.lstm = nn.LSTM(4*HIDDEN_SIZE, HIDDEN_SIZE, 1, bidirectional=False, dropout=DROPOUT)\n",
        "\n",
        "\n",
        "  def forward(self, U, max_iter):\n",
        "\n",
        "    # TODO: Value to choose for max_iter (600?)\n",
        "    # Initialise h_0, s_i_0, e_i_0 (TODO can change)\n",
        "    s = 0\n",
        "    e = 0\n",
        "\n",
        "    # initialize the hidden and cell states \n",
        "    # hidden = (h, c)\n",
        "\n",
        "    doc_length = U.size()[1]\n",
        "    hidden = (th.randn(1,1,HIDDEN_SIZE), th.randn(1,1,HIDDEN_SIZE))\n",
        "    \n",
        "    # \"The iterative procedure halts when both the estimate of the start position \n",
        "    # and the estimate of the end position no longer change, \n",
        "    # or when a maximum number of iterations is reached\"\n",
        "\n",
        "    # We build up the losses here (the iteration being the first dimension)\n",
        "    alphas = th.tensor([]).view(0, doc_length)\n",
        "    betas = th.tensor([]).view(0, doc_length)\n",
        "\n",
        "    # TODO: make it run only until convergence (or maxiter)\n",
        "    for _ in range(max_iter):\n",
        "      # call LSTM to update h_i\n",
        "\n",
        "      # Step through the sequence one element at a time.\n",
        "      # after each step, hidden contains the hidden state.\n",
        "      u_concatenated = th.cat((U[:,s], U[:,e]), dim=0).view(-1, 1)\n",
        "      #print(\"u_concatenated.size()\", u_concatenated.size())\n",
        "\n",
        "      out, hidden = self.lstm(u_concatenated.view(1, 1, -1), hidden)\n",
        "      h, _ = hidden\n",
        "\n",
        "      # Call HMN to update s_i, e_i\n",
        "      alpha = th.tensor([]).view(0, 1)\n",
        "      beta = th.tensor([]).view(0, 1)\n",
        "\n",
        "      for t in range(doc_length):\n",
        "        t_hmn_alpha = self.hmn_alpha(U[:, t].view(-1, 1), h.view(-1, 1), U[:,s].view(-1, 1), U[:,e].view(-1, 1))\n",
        "        t_hmn_beta = self.hmn_beta(U[:, t].view(-1, 1), h.view(-1, 1), U[:,s].view(-1, 1), U[:,e].view(-1, 1))\n",
        "        alpha = th.cat((alpha, t_hmn_alpha.view(1, 1)), dim=0)\n",
        "        beta = th.cat((beta, t_hmn_beta.view(1, 1)), dim=0)\n",
        "\n",
        "      # Leaving out dim=0 changes behaviour\n",
        "      _, s = th.max(alpha, dim=0)\n",
        "      _, e = th.max(beta, dim=0)\n",
        "      \n",
        "      alphas = th.cat((alphas, alpha.view(1, -1)), dim=0)\n",
        "      betas = th.cat((betas, beta.view(1, -1)), dim=0)\n",
        "\n",
        "    return (alphas, betas, s, e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJLTzYCxvYNI",
        "colab_type": "code",
        "outputId": "c630e47c-ca92-4a02-ca2b-9d75ecaf6efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "\n",
        "dpd = DynamicPointerDecoder()\n",
        "max_iter = 10\n",
        "U = th.ones(2 * HIDDEN_SIZE, 50)\n",
        "alphas, betas, s, e = dpd.forward(U, max_iter)\n",
        "loss = th.mean(th.mean(alphas, dim=0)) + th.mean(th.mean(betas, dim=0))\n",
        "loss.backward()\n",
        "print(loss)\n",
        "#for param in dpd.parameters():\n",
        "#  print(param.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(114774.6719, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqEnQPV9O_Yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The full model.\n",
        "class DCNModel(nn.Module):\n",
        "  def __init__(self, doc_word_vecs, question_word_vecs, encoder_h_dim, encoder_dropout):\n",
        "    super(DCNModel, self).__init__()\n",
        "    self.encoder_h_dim = encoder_h_dim\n",
        "    self.encoder = Encoder(doc_word_vecs, question_word_vecs, encoder_h_dim, encoder_dropout)\n",
        "    print(\"Encoder shape: \",self.encoder)\n",
        "    self.encoder_sentinel = nn.Parameter(th.randn(encoder_h_dim)) # the sentinel is a trainable parameter of the network\n",
        "    self.WQ = nn.Linear(encoder_h_dim, encoder_h_dim)\n",
        "    self.dyn_ptr_dec = DynamicPointerDecoder() \n",
        "    self.coattention = CoattentionModel(encoder_h_dim, encoder_dropout)\n",
        "\n",
        "  def forward(self, doc_word_vecs, question_word_vecs):\n",
        "    \n",
        "    # TODO: how should we initialise the hidden state of the LSTM? For now:\n",
        "    hidden = self.encoder.generate_initial_hidden_state()\n",
        "    for i in doc_word_vecs:\n",
        "        outp, hidden = self.encoder.lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "    D = th.cat([outp.view(-1), self.encoder_sentinel], 0)  # append sentinel word vector\n",
        "\n",
        "    # TODO: Make sure we should indeed reinit hidden state before encoding the q.\n",
        "    hidden = self.encoder.generate_initial_hidden_state()\n",
        "    for i in question_word_vecs:\n",
        "        outp, hidden = self.encoder.lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "    Qprime = th.cat([outp.view(-1), self.encoder_sentinel], 0)  # append sentinel word vector\n",
        "    Q = th.tanh(self.WQ(Qprime.view(-1, self.encoder_h_dim))).view(Qprime.size())\n",
        "\n",
        "    U = self.coattention.forward(D.unsqueeze(dim=0),Q.unsqueeze(dim=0))\n",
        "    \n",
        "    # TODO: Replace with the true start/end positions for the current batch of questions\n",
        "    x,y = th.randint(0, doc_word_vecs.size()[0], (2,))\n",
        "    x, y = min(x,y), max(x,y)\n",
        "    \n",
        "    # TODO: Replace with actual U\n",
        "    max_iters = 10\n",
        "    doc_words = doc_word_vecs.size()[0]\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()    \n",
        "\n",
        "    # Run the dynamic pointer decoder, accumulate the \n",
        "    # distributions over the start positions (alphas)\n",
        "    # and end positions (betas) at each iteration\n",
        "    # as well as the final start/end indices \n",
        "    \n",
        "    alphas, betas, start, end = self.dyn_ptr_dec.forward(U, max_iters)\n",
        "\n",
        "    # Accumulator for the losses incurred across \n",
        "    # iterations of the dynamic pointing decoder\n",
        "    loss = th.FloatTensor([0])\n",
        "    for it in range(max_iters):\n",
        "      loss += criterion(alphas[it].view(1, -1), Variable(th.LongTensor([x])))\n",
        "      loss += criterion(betas[it].view(1, -1), Variable(th.LongTensor([x])))\n",
        " \n",
        "    return loss, start, end\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-wwXuSlP0Sx",
        "colab_type": "code",
        "outputId": "532b4492-3ed6-425f-8f2d-0d448f4e80b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "source": [
        "# TEST\n",
        "# DCNModel Test.\n",
        "doc = th.randn(30, ENCODING_DIM)\n",
        "que = th.randn(5, ENCODING_DIM)\n",
        "\n",
        "# Run model.\n",
        "model = DCNModel(doc, que, ENCODING_DIM, DROPOUT)\n",
        "loss, s, e = model.forward(doc, que)\n",
        "print(\"loss: \", loss, \", s:\", s, \", e:\", e)\n",
        "model.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "print(\"%d/%d parameters are not None.\" % (len([param for param in model.parameters() if param is not None]), len(list(model.parameters()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder shape:  Encoder(\n",
            "  (lstm): LSTM(200, 200, dropout=0.3)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-9ce86840a172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mque\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENCODING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", s:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", e:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-b6f5056c9669>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, doc_word_vecs, question_word_vecs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQprime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_h_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQprime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# TODO: Replace with the true start/end positions for the current batch of questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-8f1ae4d74bcc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, D, Q)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#co attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mD_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#B x l x m + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# L = B x m + 1 x n + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mAQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXbwCLjVIxm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2FPkvEDZGf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimiser \n",
        "\n",
        "doc = th.ones(30, ENCODING_DIM)\n",
        "que = th.ones(5, ENCODING_DIM)\n",
        "model = DCNModel(doc, que, ENCODING_DIM, 0.3)\n",
        "\n",
        "# TODO: hyperparameters?\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "n_iters = 1000 \n",
        "\n",
        "# TODO: batching? \n",
        "for iter in range(n_iters):\n",
        "  optimizer.zero_grad()\n",
        "  loss, _, _ = model(doc, que)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1WgxkSHntYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST\n",
        "# HMN test\n",
        "\n",
        "hmn = HighwayMaxoutNetwork()\n",
        "u_t = th.ones(2 * ENCODING_DIM, 1)\n",
        "h_i = th.ones(ENCODING_DIM, 1)\n",
        "u_si_m_1, u_ei_m_1 = th.ones(2 * ENCODING_DIM, 1), th.ones(2 * ENCODING_DIM, 1)\n",
        "output = hmn.forward(u_t, h_i, u_si_m_1, u_ei_m_1)\n",
        "#print(output)\n",
        "output.backward()\n",
        "\n",
        "#for param in hmn.parameters(): \n",
        "#  print(param.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5do8F5yj9ma4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST\n",
        "# Dynamic pointer decoder test\n",
        "dpe = DynamicPointerDecoder()\n",
        "alphas, betas, _, _ = dpe.forward(th.randn(2 * HIDDEN_SIZE, 50), 10)\n",
        "dpe.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZUnpXhnRVqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1zCRVN4wGyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}